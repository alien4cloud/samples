### COMMONS ###
cls.bigdata.core {
  impala {
    connection-string = "jdbc:impala://192.168.1.22:21050;AuthMech=0"
  }
  hdfs {
    fs.defaultFS = "hdfs://hdfs-1.novalocal:8022"
    dfs.client.use.datanode.hostname = "true"
  }
  kafka {
    bootstrap.servers = "${KAFKA_IP}:${KAFKA_PORT}"
  }
  ### REDIS ###
  redis {
    hostname = "192.168.1.53"
    port = 6379
  }
  ### KUDU ###
  kudu {
    master-addresses = "kudumast-1.novalocal:7051"
  }
  data {
    impala {
      database-name = "artemis"
      tables {
        vessel-registry-by-day = ${cls.bigdata.core.data.impala.database-name}".vessel_registry_by_day"
        vid-mmsi-imo = ${cls.bigdata.core.data.impala.database-name}".vidmmsiimo"
        position = ${cls.bigdata.core.data.impala.database-name}".position"
        last-position-by-day = ${cls.bigdata.core.data.impala.database-name}".last_position_by_day"
      }
    }
    stores {

      # Set in the script ro run the job (run-spark.sh / dcos)
      #  merge-parquet = true
      merge-parquet = false

      # Set in the script ro run the job (run-spark.sh / dcos)
      #  root.dir="/artemis/data/ais"
      root.dir="hdfs:///aisliv/data/ais"

      mid.dir=${cls.bigdata.core.data.stores.root.dir}"/mid/enhanced"

      raw1.dir=${cls.bigdata.core.data.stores.root.dir}"/raw1"

      raw2.dir=${cls.bigdata.core.data.stores.root.dir}"/raw2"
      vidmmsiimo.dir=${cls.bigdata.core.data.stores.root.dir}"/vidmmsiimo"

      position.dir=${cls.bigdata.core.data.stores.root.dir}"/position"

      vessel.dir=${cls.bigdata.core.data.stores.root.dir}"/vessel_registry"
      vessel-by-day.dir=${cls.bigdata.core.data.stores.root.dir}"/vessel_registry_by_day"
      voyage.dir=${cls.bigdata.core.data.stores.root.dir}"/voyage_registry"
      voyage-by-day.dir=${cls.bigdata.core.data.stores.root.dir}"/voyage_registry_by_day"

      geofencing-archive.dir=${cls.bigdata.core.data.stores.root.dir}"/geofencing_archive"

      distance-to-shore.file=${cls.bigdata.core.data.stores.root.dir}"/distance_to_shore/dst-100.bin"
      tle.file=${cls.bigdata.core.data.stores.root.dir}"/tle/tleData.dat"


    }
  }
}

### AKKA ###
akka.loglevel = DEBUG

akka.kafka.consumer.kafka-clients {
  enable.auto.commit = false
  bootstrap.servers = ${cls.bigdata.core.kafka.bootstrap.servers}
  group.id = "ais-consumer-group"
}


### POSITION ###
cls.bigdata.core.position {
  days-in-kudu = 30

  impala {
    parquet {
      table = ${cls.bigdata.core.data.impala.database-name}".position"
      last-position-table = ${cls.bigdata.core.data.impala.database-name}".last_position_by_day"
    }
  }

  kudu {
    table = "impala::"${cls.bigdata.core.data.impala.database-name}".recent_position"
    last-position-table = "impala::"${cls.bigdata.core.data.impala.database-name}".last_position"
  }
}

### SERVICES ###
cls.bigdata.core.service {

  vesselhisto.table = ${cls.bigdata.core.data.impala.database-name}".vessel_registry"
  vesselhisto.table-by-day = ${cls.bigdata.core.data.impala.database-name}".vessel_registry_by_day"

  voyage {
    declarative {
      table = ${cls.bigdata.core.data.impala.database-name}".voyage_registry"
    }
    realtime {
      table = ${cls.bigdata.core.data.impala.database-name}".geofencing"
    }
  }

  ports.table = ${cls.bigdata.core.data.impala.database-name}".ports"

}

### AIS raw1 converter ###
cls.bigdata.core.ais.raw1.converter {

  # Mode: daemon
  mode = "daemon"

  # Flag that indicates if the conversion from raw0 to raw1 shall be done
  convert-from-raw0-to-raw1 = true

  # Flag that indicates if the copy from local incoming directory to HDFS incoming directory shall be done
  copy-from-local-to-hdfs = false

  # Extension used for complete raw0 archive
  raw0-archive-extension = ".zip"

  hdfs-dirs {


    # Home directory of the raw0 data (including the sub directories incoming,inprogress,converted,failed)
    raw0-home-dir = "/aisliv/data/ais/raw0"

    # Sub directory of raw0-home-dir in which incoming raw0 are put to be converted into raw1
    raw0-incoming-subdir = "incoming"

    # Sub directory of raw0-home-dir in which inprogress raw0 are put where there are converted into raw1
    raw0-inprogress-subdir = "inprogress"

    # Sub directory of raw0-home-dir in which converted raw0 are put when they have been converted into raw1
    raw0-converted-subdir = "converted"

    # Sub directory of raw0-home-dir in which failed raw0 are put when they the conversion into raw1 has failed
    raw0-failed-subdir = "failed"

    # Directory of the raw1 data (into which the raw0 are converted into raw1)
    raw1-dir = "/aisliv/data/ais/raw1"

    # Overwrite raw1
    raw1-overwrite = true

    # Extension used during the copy from local ingestion incoming directory to HDFS
    raw0-tmp-copy-extension = ".tmp"

  }

}


### AIS ###
cls.bigdata.core.ais {
  system-termination-timeout = 10 s

  replay-tool {
    from-instant = "2018-04-20T00:04:02.000"
    from-instant = ${?REPLAY_FROM}
    to-instant = "2018-04-20T00:04:04.000"
    to-instant = ${?REPLAY_TO}
    speed-factor = 0.1

    incoming-dir = "file:///ais/input"

    outgoing-dir = "file:///ais/outgoing"

    delete-files = false
    copy-files = true

    update-send-ts = true

    kafka {
      topic = "ais-replay"

      kafka-client {
        # Kafka bootstrap servers
        bootstrap.servers = ${cls.bigdata.core.kafka.bootstrap.servers}
      }
    }

    zipping-enabled = true
    zipping-dir = "file:///ais/zip"
    zipping-period = 1m
  }

  ### AIS producer ###
  producer {
    visitor = "ais-replay"
    sink = "kafka"

    visitors {
      ais-replay {
        topic = "${INPUT_TOPIC}"

        # Consumer polling timeout
        polling-timeout = 3 seconds

        # Max amount of records to remain in pending state, before the consumer is paused
        max-pending-tasks = 100

        # Max number of uncommitted kafka records before a commit is forced
        max-uncommitted-tasks = 10
        # Max amount of time to wait between 2 consecutive commits
        commit-period = 10 seconds

        kafka-client {
          # Kafka bootstrap servers
          bootstrap.servers = ${cls.bigdata.core.kafka.bootstrap.servers}

          group.id = "ais-replay-group"
          auto.offset.reset = "earliest"
          enable.auto.commit = false
        }
      }
    }

    sinks {
      kafka {
        topic = "${OUTPUT_TOPIC}"

        kafka-client {
          # Kafka bootstrap servers
          bootstrap.servers = ${cls.bigdata.core.kafka.bootstrap.servers}
        }
      }
    }
  }

  ### Consumer ###
  consumer {
    extensions {

      qualifiers {
        common-qualifier {
          enabled = true
          max-instant-speed = 35
          tle-path = "/aisliv/data/ais/tle/tleData.dat"
          # Maximum speed in knot (calculated between current and previous position) to cosider a position as valid
          max-instant-speed = 200
        }
      }

      event-processors {
        kudu-ais-position-persistor {
          enabled = true
          table = "impala::"${cls.bigdata.core.data.impala.database-name}".recent_position"
          always-flush = true
        }
        kudu-ais-last-position-persistor {
          enabled = true
          table = "impala::"${cls.bigdata.core.data.impala.database-name}".last_position"
        }
        cmaTerminalWatcher {
          enabled = false
          update-period = 0minute # Never done

          # business configuration done in cls.bigdata.shipping
          #					. terminalwatcher
          #					. terminal
          #					. fleet (deprecated)
          #					. cmaschedule

        }
      }

    }

    kafka {
      topic = "${OUTPUT_TOPIC}"
    }

    partitions {

      commit {
        # Maximum amount of offset that the commit actor can wait before attempting a commit.
        offsets-buffer-max-size = 1000

        # Maximum time that the commit actor should wait in the collecting state before attempting a commit.
        collect-offsets-period = 10 second

        # Maximum amount of time that the commit actor should wait for missing offsets before forcing a commit (and ignoring the missing offsets).
        wait-missing-offsets-timeout = 1 minutes
      }
    }

    vessel {
      # Maximum amount of time the vessel actors keeps the ais message processing result in memory before flushing
      max-buffering-time = 5 minutes

      # Maximum amount of time the vessel actors can stay alive after the last received ais message (should be greater than `max-buffering-time`).
      max-idle-time = 7 minute
    }

  }
}

cls.bigdata.core.service {
	berthunberth.table =  ${cls.bigdata.core.data.impala.database-name}".berthunberth"
	
	portreceptionanomaly.table =  ${cls.bigdata.core.data.impala.database-name}".portreceptionanomaly"
	portreceptionanomaly.ports-table = ${cls.bigdata.core.data.impala.database-name}".ports"
	
	vesselhisto.table =  ${cls.bigdata.core.data.impala.database-name}".vessel_registry"
	vesselhisto.table-by-day =  ${cls.bigdata.core.data.impala.database-name}".vessel_registry_by_day"
	voyage {
		declarative {
			table =  ${cls.bigdata.core.data.impala.database-name}".voyage_registry"
		}
		realtime {
			table = ${cls.bigdata.core.data.impala.database-name}".geofencing"
			table-archive = ${cls.bigdata.core.data.impala.database-name}".geofencing_archive"
		}
	}
	
	distancetoshore.bin-path = "/aisliv/data/ais/distance_to_shore/dst-100.bin"
	mid.table = ${cls.bigdata.core.data.impala.database-name}".mid"
	ports.table = ${cls.bigdata.core.data.impala.database-name}".ports"
	region.table = ${cls.bigdata.core.data.impala.database-name}".zones"
	
	atseaencounter.table = ${cls.bigdata.core.data.impala.database-name}".atseaencounter"
	atseaencounter.max-days-duration = 5
}

### UDA Bigdata ###
cls.bigdata.uda.masbgd {
  application = "masbgd"
  port = 8080

  positions {
    limit = 50000
  }

  repositories {
    mobile = true
    position = true
    vesselTimeline = true
  }
}
